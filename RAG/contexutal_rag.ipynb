{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_groq'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_groq'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json \n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "from glob import glob\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\bhavi\\OneDrive\\Desktop\\langhcain_learning\\RAG\\rag_docs\\wikidata_rag_demo.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(file_path=file_path,\n",
    "                    jq_schema=\".\",\n",
    "                    text_content=False,\n",
    "                    json_lines=True)\n",
    "\n",
    "wiki_docs = loader.load()\n",
    "print(\"LENGTH OF DOCS ----\",len(wiki_docs))\n",
    "print(wiki_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading ddata from json \n",
    "wikipedia_documents = []\n",
    "\n",
    "for doc in wiki_docs:\n",
    "    doc = json.loads(doc.page_content)\n",
    "    meta_data = {\"title\":doc[\"title\"],\n",
    "                 \"id\":doc[\"id\"],\n",
    "                 \"source\":\"wikipedia\",\n",
    "                 \"page\":1\n",
    "                 }\n",
    "    \n",
    "    data = \" \".join(doc[\"paragraphs\"])\n",
    "    wikipedia_documents.append(Document(page_content=data ,metadata=meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model \n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\",\n",
    "                 temperature=0,\n",
    "                 max_tokens=None,\n",
    "                 api_key=api_key,\n",
    "                 timeout=None,\n",
    "                 max_retries=2,\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_chunks(file_path, chunk_size=1500, chunk_overlap=150):\n",
    "    print(\"Loading Pages:\", file_path)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "\n",
    "    print(\"Chunking pages...\", file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    doc_chunks = splitter.split_documents(doc_pages)\n",
    "\n",
    "    standard_chunks = []\n",
    "\n",
    "    for chunk in doc_chunks:\n",
    "        chunk_metadata_upd = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"page\": chunk.metadata.get(\"page\"),\n",
    "            \"source\": file_path,\n",
    "            \"title\": os.path.basename(file_path) \n",
    "        }\n",
    "\n",
    "        standard_chunks.append(Document(\n",
    "            page_content=chunk.page_content,\n",
    "            metadata=chunk_metadata_upd\n",
    "        ))\n",
    "        \n",
    "    print(\"Finished processing --------\", file_path)\n",
    "    return standard_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob(\"C:/Users/bhavi/OneDrive/Desktop/langhcain_learning/RAG/rag_docs/*.pdf\")\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_docs = []\n",
    "\n",
    "for fp in pdf_files:\n",
    "    paper_docs.extend(create_standard_chunks(file_path=fp,chunk_size=1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chunks = wikipedia_documents + paper_docs\n",
    "print(\"------ Lenght of Documents ---------\",len(total_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing Documents and chunk embeddings in Vector DB \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en')\n",
    "\n",
    "chroma_db = Chroma.from_documents(documents=total_chunks,\n",
    "                                  embedding=embedding_model,\n",
    "                                  collection_metadata={\"hnsw:space\":\"cosine\"},\n",
    "                                  persist_directory=\"./wikipedia_db\")\n",
    "\n",
    "print(\"[----EMBEDDINGS CREATED ---------]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing similarity based retrieval \n",
    "from langchain_community.retrievers import BM25Retriever \n",
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\":5})\n",
    "\n",
    "bm25_retrievers = BM25Retriever.from_documents(documents=total_chunks,\n",
    "                                               k=5)\n",
    "print(\"--- Similarity and bm25 Retriever initalizes ----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ensemble Retriever\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "\n",
    "print(any(m.name == \"langchain_community\" for m in pkgutil.iter_modules()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
